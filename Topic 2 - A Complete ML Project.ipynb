{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwdmik0ARKfm"
      },
      "source": [
        "**Topic 2 – End-to-end Machine Learning project**\n",
        "\n",
        "*Welcome to ECIC Housing Development Ltd! Your task is to predict median house values in Californian districts, given a number of features from these districts.*\n",
        "\n",
        "This is a very long notebook.  It will take many minutes to complete executing the codes in this entire notebook.  For fun, the textbok called the company Machine Learning Housing Corp.  I call it ECIC Housing Development Ltd.  It is not a real company.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KENvUfbIRKfr"
      },
      "source": [
        "# Setup\n",
        "Remember to import all the libraries/modules/classes used in your Python prgram before using them.  The import does not need to be in the beginning, but has to be imported before using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtjazubuRKfs"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LlXpHkdvIMRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy15sLXXRKfs"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 and Scikit-Learn ≥0.20 are required\n",
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures (for report etc)\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEriVE-nRKfu"
      },
      "source": [
        "# Get the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PzBhWTLRKfu"
      },
      "source": [
        "## Download the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjTp0iULRKfv"
      },
      "outputs": [],
      "source": [
        "# It is a good practice to \"package\" your tasks in functions!\n",
        "import os   # does not matter if you import twice (just in case!)\n",
        "import tarfile  # for de-compress tarfiles\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    if not os.path.isdir(housing_path):\n",
        "        os.makedirs(housing_path)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx08riO0RKfw"
      },
      "outputs": [],
      "source": [
        "fetch_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1C6FCI6RKfw"
      },
      "outputs": [],
      "source": [
        "# We shall be using pandas DataFrame to manage all our data\n",
        "# Again, use function for clear documentation and modularization\n",
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKQzdlJ7RKfx"
      },
      "source": [
        "## Take a Quick Look at the Data Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otDsIC3oRKfx"
      },
      "outputs": [],
      "source": [
        "# Useful to get a \"peek\" at the data using .head()\n",
        "housing = load_housing_data()\n",
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0QYM2p8RKfy"
      },
      "outputs": [],
      "source": [
        "# Useful to know what you have in the DataFrame\n",
        "# The structure is MORE important than the actual data!!\n",
        "housing.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jRCmezsRKfy"
      },
      "outputs": [],
      "source": [
        "# As this column is non-numerical, useful to see what it is!\n",
        "housing[\"ocean_proximity\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5CMgmd5RKfz"
      },
      "outputs": [],
      "source": [
        "# For numeric data - get an overall statisitics!\n",
        "housing.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0claTC0ZRKfz"
      },
      "outputs": [],
      "source": [
        "# Get a histograph plot for numeric data to have a big picture of your data\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "save_fig(\"attribute_histogram_plots\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ioVRWL2RKfz"
      },
      "source": [
        "## Create a Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br4wJ9rnRKf0"
      },
      "outputs": [],
      "source": [
        "# We do random sampling, hence it is important to make the out\n",
        "# (repeatibility) same every time we run but give a fix seed (42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7CaaLTcRKf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# For illustration only. Sklearn has train_test_split()\n",
        "# Again, package the task in a function!!!!!!!!\n",
        "def split_train_test(data, test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG5S-I1JRKf0"
      },
      "outputs": [],
      "source": [
        "# Get the training set and testing set with one call!\n",
        "# Print the \"info\" of each dataset\n",
        "train_set, test_set = split_train_test(housing, 0.2)\n",
        "print(\"Training - \")\n",
        "train_set.info()\n",
        "print(\"Testing - \")\n",
        "test_set.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9YtZW-iRKf1"
      },
      "outputs": [],
      "source": [
        "# create a random ID for each instance for easier management later\n",
        "from zlib import crc32\n",
        "\n",
        "def test_set_check(identifier, test_ratio):\n",
        "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
        "\n",
        "def split_train_test_by_id(data, test_ratio, id_column):\n",
        "    ids = data[id_column]\n",
        "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
        "    return data.loc[~in_test_set], data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBTl8-hRKf1"
      },
      "source": [
        "Cycle Redundancy Check is a fine but may not work if you want to add additional data later.  You would have to redo everything from the beginning.  A better alternative is to use a Hash function.\n",
        "The following code is easier than what is used in the textbook, by using the md5 hash function from hashlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GgcucX6RKf1"
      },
      "outputs": [],
      "source": [
        "# using Hash Function is an even better option for sampling\n",
        "import hashlib\n",
        "\n",
        "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
        "    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N30OxfyRKf2"
      },
      "outputs": [],
      "source": [
        "# we need to add the `index` column to our datasets\n",
        "housing_with_id = housing.reset_index()   # adds an `index` column\n",
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# have a look at the test_set (can do the same for train_set)\n",
        "test_set.info()"
      ],
      "metadata": {
        "id": "9JK45eLDABqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdimhrLyRKf2"
      },
      "outputs": [],
      "source": [
        "# yet another alternative of ID is using 'longitude' and 'latitude'\n",
        "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA1YfdqWRKf2"
      },
      "outputs": [],
      "source": [
        "# look at the first instances of test_set using .head()\n",
        "test_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7-lFcBlRKf2"
      },
      "outputs": [],
      "source": [
        "# all the above are for explanation - use sklearn's train_test_split!!!!\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfwppWnBRKf2"
      },
      "outputs": [],
      "source": [
        "test_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZyRHElFRKf3"
      },
      "outputs": [],
      "source": [
        "# experts tell you 'median_income' is most important feature, have a look\n",
        "housing[\"median_income\"].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-6mdr4RRKf3"
      },
      "outputs": [],
      "source": [
        "# you want to see if you need to stratify your data according to `median_income`\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCdW8qOFRKf3"
      },
      "outputs": [],
      "source": [
        "# look at each bin\n",
        "housing[\"income_cat\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6dW62DCRKf3"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0I-0_2iRKf4"
      },
      "outputs": [],
      "source": [
        "# yes, you need to stratify, use StratifiedShuffleSplit class in sklearn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the index for stratification is an numpy ndarray!\n",
        "type(train_index)"
      ],
      "metadata": {
        "id": "gXYQCXkqDNpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tiHWge-RKf4"
      },
      "outputs": [],
      "source": [
        "# check if the stratification agrees with the distribution!!\n",
        "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QErI1w6RKf4"
      },
      "outputs": [],
      "source": [
        "# Great!!  They agree!\n",
        "housing[\"income_cat\"].value_counts() / len(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIDxyVeRKf4"
      },
      "outputs": [],
      "source": [
        "# this is nothing to do with our model, but to see the effect of stratification!\n",
        "def income_cat_proportions(data):\n",
        "    return data[\"income_cat\"].value_counts() / len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall\": income_cat_proportions(housing),\n",
        "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
        "    \"Random\": income_cat_proportions(test_set),\n",
        "}).sort_index()\n",
        "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
        "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6VYEs98RKf4"
      },
      "outputs": [],
      "source": [
        "compare_props"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICSFrx_4RKf4"
      },
      "outputs": [],
      "source": [
        "# the 'income_cat' column is used only for stratification (in sampling)\n",
        "# now that we have the stratified training and testing set, drop that column!\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-e28VBXRKf5"
      },
      "source": [
        "# Discover and Visualize the Data to Gain Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgzx2m67RKf5"
      },
      "outputs": [],
      "source": [
        "# make a copy of the stratified training dataset in 'housing' (our training DataFrame)\n",
        "# all the work so far is to get the right set of data!!\n",
        "housing = strat_train_set.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2gJwTj5RKf5"
      },
      "source": [
        "## Visualizing Geographical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj5cy_HtRKf5"
      },
      "outputs": [],
      "source": [
        "# essential to get a picture of the data we are dealing with - visualization!\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
        "save_fig(\"bad_visualization_plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpwQTaKORKf5"
      },
      "outputs": [],
      "source": [
        "# demonstrate use of 'alpha' (transparency) in plots\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
        "save_fig(\"better_visualization_plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osqRotq4RKf5"
      },
      "source": [
        "The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1R5XvnvRKf5"
      },
      "outputs": [],
      "source": [
        "# use size of circles ('s') for population and colour ('c') for median house value\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
        "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
        "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
        "             sharex=False)\n",
        "plt.legend()\n",
        "save_fig(\"housing_prices_scatterplot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFZE7aQrRKf5"
      },
      "outputs": [],
      "source": [
        "# Download the California image\n",
        "# this is not in the book or the powerpoint notes - extravagent!!\n",
        "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "filename = \"california.png\"\n",
        "print(\"Downloading\", filename)\n",
        "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
        "urllib.request.urlretrieve(url, os.path.join(images_path, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzD4VJzRRKf6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
        "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
        "                  s=housing['population']/100, label=\"Population\",\n",
        "                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
        "                  colorbar=False, alpha=0.4)\n",
        "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
        "           cmap=plt.get_cmap(\"jet\"))\n",
        "plt.ylabel(\"Latitude\", fontsize=14)\n",
        "plt.xlabel(\"Longitude\", fontsize=14)\n",
        "\n",
        "prices = housing[\"median_house_value\"]\n",
        "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
        "cbar = plt.colorbar(ticks=tick_values/prices.max())\n",
        "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
        "cbar.set_label('Median House Value', fontsize=16)\n",
        "\n",
        "plt.legend(fontsize=16)\n",
        "save_fig(\"california_housing_prices_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqV3wR6cRKf6"
      },
      "source": [
        "## Looking for Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfBjrZP5RKf6"
      },
      "outputs": [],
      "source": [
        "# get the correlaton matrix - this is how to see which attributes are more useful\n",
        "corr_matrix = housing.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYT9GHbGRKf6"
      },
      "outputs": [],
      "source": [
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en_H8eM2RKf6"
      },
      "outputs": [],
      "source": [
        "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "save_fig(\"scatter_matrix_plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbrBMzDpRKf7"
      },
      "outputs": [],
      "source": [
        "# 'median_income' by far is most correlated and useful, have a closer look!\n",
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
        "             alpha=0.1)\n",
        "plt.axis([0, 16, 0, 550000])\n",
        "save_fig(\"income_vs_house_value_scatterplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChdQzVE6RKf7"
      },
      "source": [
        "## Experimenting with Attribute Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e36z9ifpRKf7"
      },
      "outputs": [],
      "source": [
        "# Experimenting our attributes is important to get max benefit from data.\n",
        "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
        "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
        "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzZg5OQORKf7"
      },
      "outputs": [],
      "source": [
        "# experiment shows `bedrooms_per_room` is significant (negatively correlated!)\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KbMphclRKf7"
      },
      "outputs": [],
      "source": [
        "# 'room_per_household' is also useful (but not as significant as 'bedrooms_per_room')\n",
        "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
        "             alpha=0.2)\n",
        "plt.axis([0, 5, 0, 520000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvYKp9OIRKf7"
      },
      "outputs": [],
      "source": [
        "# check out the statistics of `housing` DataFrame\n",
        "housing.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eG6wxLORKf7"
      },
      "source": [
        "# Prepare the Data for Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBP3EAQwRKf8"
      },
      "outputs": [],
      "source": [
        "# drop the labels from the training dataset and set up a separate `housing_labels1 dataset\n",
        "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YDUCyIuRKf8"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwEho6JORKf8"
      },
      "source": [
        "In the book 3 options for data cleaning are listed:\n",
        "\n",
        "```python\n",
        "housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n",
        "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
        "median = housing[\"total_bedrooms\"].median()  # option 3\n",
        "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
        "```\n",
        "\n",
        "To demonstrate each of them, let's create a copy of the housing dataset, but keeping only the rows that contain at least one null. Then it will be easier to visualize exactly what each option does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvPi_v8tRKf8"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqu_wxWeRKf8"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUTbjfXdRKf8"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWJgez_4RKf8"
      },
      "outputs": [],
      "source": [
        "median = housing[\"total_bedrooms\"].median()\n",
        "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D0t7u8PRKf9"
      },
      "outputs": [],
      "source": [
        "sample_incomplete_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyLBTsO9RKf9"
      },
      "outputs": [],
      "source": [
        "# at the end, let us use the 'SimpleImputer' of sklearn and \"median\" strategy to clean data\n",
        "# the above are to explain what could be done using available packages, not by a single call!\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABLWknRTRKf9"
      },
      "source": [
        "Remove the text attribute because median can only be calculated on numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrL5bTQdRKf9"
      },
      "outputs": [],
      "source": [
        "# but wait... median can only be applied to numerical attributes, hence need to do more...\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
        "# alternatively: housing_num = housing.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK_w-Yo2RKf9"
      },
      "outputs": [],
      "source": [
        "# now apply the .fit() method to housing_num (numerical subset of housing)\n",
        "# you will learn about .fit(), .transform() and .fit_transform() in lecture\n",
        "# these concepts are important!\n",
        "imputer.fit(housing_num)\n",
        "# the result of .fit() method is to get the statistics from the dataset (fitting the data!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRNkWSi80mNM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBB3gkY_RKf9"
      },
      "outputs": [],
      "source": [
        "# the results are store in the variable 'statistics_'\n",
        "# lst us have a look at it\n",
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazzcPt1RKf-"
      },
      "source": [
        "Check that this is the same as manually computing the median of each attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIwK3jQsRKf-"
      },
      "outputs": [],
      "source": [
        "# the same as computing the median fore each attribute\n",
        "housing_num.median().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjasx3skRKf-"
      },
      "source": [
        "Transform the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oqj3MLYRKf-"
      },
      "outputs": [],
      "source": [
        "# Now!!  Ready to transform the housing_num data (using imputer) and put the transformed data in X (captial)\n",
        "X = imputer.transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7zY-0jJRKf-"
      },
      "outputs": [],
      "source": [
        "# now, get the DataFrame from X, and setup the `housing_tr` - the transformed housing dataset\n",
        "# the follwing code cells are just to examine some of the transformed data in `housing_tr`\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wml5-pdMRKf-"
      },
      "outputs": [],
      "source": [
        "housing_tr.loc[sample_incomplete_rows.index.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUoT4LNrRKf-"
      },
      "outputs": [],
      "source": [
        "imputer.strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ua45CowRKf-"
      },
      "outputs": [],
      "source": [
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing_num.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIjCs9M-RKf-"
      },
      "outputs": [],
      "source": [
        "housing_tr.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8DB3OMURKf_"
      },
      "source": [
        "## Handling Text and Categorical Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AZTT5xERKf_"
      },
      "source": [
        "After preprocessing numerical data using Imputer and set up our transformers, now let's preprocess the **categorical input feature**, `*ocean_proximity*`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INUW-QldRKf_"
      },
      "outputs": [],
      "source": [
        "# have a quick look at `ocean_proximity` atrribute in the first 10 instances \n",
        "# 'housing_cat' is similar to 'housing_num\" but for categorical data\n",
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfxyfpdfRKf_"
      },
      "outputs": [],
      "source": [
        "# need to encode description 'strings' into a datastructure that is meaningful to computer\n",
        "# use 'OrdinalEncoder' class in 'preprocessing' module of sklearn\n",
        "# note that .fit_transform does 2 things together - fitting, then transform\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]  # you can see that the encoded is an numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqp3841uRKf_"
      },
      "outputs": [],
      "source": [
        "# orindal transform assumes certain ordinal order, but this is not our case\n",
        "# '<1H OCEAN' is not better or worst than 'INLAND', look at the categories\n",
        "ordinal_encoder.categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFgF418TRKf_"
      },
      "outputs": [],
      "source": [
        "# use OneHotEncoder in sklearn solves the problem!\n",
        "# it generate a sparse matrix, each row has a '1' in the category it belongs to\n",
        "# and '0' elsewhere, hence sparse matrix is more efficient to store the information\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EysHSbSnRKgA"
      },
      "source": [
        "By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ijI_2a6RKgA"
      },
      "outputs": [],
      "source": [
        "# we can convert the sparse matrix back to a 'ordinary' dense matrix\n",
        "housing_cat_1hot.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUiKLM_RKgA"
      },
      "source": [
        "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTUy2is2RKgA"
      },
      "outputs": [],
      "source": [
        "# setting the 'sparse' parameter to 'False' is easier!\n",
        "cat_encoder = OneHotEncoder(sparse=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrzGzdiuRKgA"
      },
      "outputs": [],
      "source": [
        "cat_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kGHjuETRKgA"
      },
      "source": [
        "## Custom Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6E0m73XRKgA"
      },
      "source": [
        "Let's create a custom transformer to add extra attributes.  This is way more complex and you may not fully understand what it does.  It does not matter if you skip this for now.  Basically, if you cannot find the necessary transformers or algorithms in sklearn, you can write your own customized transformers yourself and this example shows you how this is done.\n",
        "\n",
        "We indeed use this custom transform to add new attributes to our Dataframe before we explore the various ML models.  The relevant attributes in the original dataset are with indexes (column) [3, 4, 5, 6] for [rooms, bedrooms, population, housefholds]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4BrfXSgRKgA"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household,\n",
        "                         bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETfeCLvRKgB"
      },
      "source": [
        "Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this.  'cleaner' here mean that you do not hard program it (then anything you change in future you need to remember to change the hard-code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pA2hTu8RKgB"
      },
      "outputs": [],
      "source": [
        "# cleaner approach to replace hard-coding\n",
        "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = [\n",
        "    housing.columns.get_loc(c) for c in col_names] # get the column indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP2Jm0G1RKgB"
      },
      "source": [
        "Also, `housing_extra_attribs` is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a `DataFrame`, you could run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE5pbfBCRKgB"
      },
      "outputs": [],
      "source": [
        "# putting back in the attribute names (remember, NumPy arrays have to be the same data type, hence no names!)\n",
        "# Pandas DataFrame has atribute names!\n",
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
        "    index=housing.index)\n",
        "housing_extra_attribs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpjOlwQMRKgB"
      },
      "source": [
        "## Transformation Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGeC2ZpBRKgB"
      },
      "source": [
        "Now let's build a pipeline for preprocessing the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mto0tADRKgB"
      },
      "outputs": [],
      "source": [
        "# Transformation pipelines make it clean and also clear for reusability\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0bs-A3XRKgB"
      },
      "outputs": [],
      "source": [
        "# have a look at the transformed numerical dataset in `housing_num_tr` DataFrame\n",
        "# pandas is clever in showing you a few rows and columns at the 'head' and 'tail' of the DataFrame\n",
        "# the number of rows and columns to show by default can also be changed (using appropriate parameters!)\n",
        "housing_num_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqyUcqKARKgC"
      },
      "outputs": [],
      "source": [
        "# use sklearn's 'ColumnTransform' to construct the full_pipeline\n",
        "# the final result in executing the full pipeline is now in housing_prepared\n",
        "# all the work so far is TO PREPARE the data!!!!!!\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDuo-0oCRKgC"
      },
      "outputs": [],
      "source": [
        "# looking at the data is ... not too useful!\n",
        "housing_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXbqvwwlRKgC"
      },
      "outputs": [],
      "source": [
        "# but look at it shape (and even .info() is probably more useful\n",
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9J3sSTdRKgD"
      },
      "source": [
        "# Select and Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NPFdjz9RKgD"
      },
      "source": [
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPoPICq9RKgD"
      },
      "outputs": [],
      "source": [
        "# we are now ready to try our well-prepared datasets (in Pandas DataFrame) on various model\n",
        "# try the simplest - linear regression model\n",
        "# remembers from Topic 1, .fit(X, y)?  The same here, so simply!\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "# when you run the above, nothing appears to happen!  In fact, it has finished fitting the model\n",
        "# and all relevant coeficients that define the model are store within the model!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRo-xgFBRKgE"
      },
      "outputs": [],
      "source": [
        "# let's try the full preprocessing pipeline on a few training instances\n",
        "# this is to gain some insight as to how the model is working\n",
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "# you need to transform the subset from training data (5 instances from training dataset)\n",
        "# you can see why the transformation pipeline is so important and useful\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-QL1uLFRKgE"
      },
      "source": [
        "Compare against the actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0IwFif2RKgE"
      },
      "outputs": [],
      "source": [
        "# as we have the labels for the training data, we can compare\n",
        "print(\"Labels:\", list(some_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06xRgewwRKgE"
      },
      "outputs": [],
      "source": [
        "# have a look at the sample data we are dealing with\n",
        "some_data_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH9hhMjLRKgE"
      },
      "outputs": [],
      "source": [
        "# we can use the `mean_squared_error` function in sklearn ('metrics' module)\n",
        "# then sqrt method in numpy to get RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9eEBaHARKgE"
      },
      "source": [
        "**Note**: since Scikit-Learn 0.22, you can get the RMSE directly by calling the `mean_squared_error()` function with `squared=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V2IPsAbRKgE"
      },
      "outputs": [],
      "source": [
        "# alternatively, we can computer the mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-vHltryRKgF"
      },
      "outputs": [],
      "source": [
        "# let us try another model, how about decision tree\n",
        "# this is to demonstrate how easy it is to try different models\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EtQPJMwRKgF"
      },
      "outputs": [],
      "source": [
        "# ZERO ERROR, something must be wrong (characteristic of this model!)\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAULaE3ARKgF"
      },
      "source": [
        "## Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc8uDC1pRKgF"
      },
      "outputs": [],
      "source": [
        "# using training data this way to validate the tune models in not good\n",
        "# this is because the model was trained WITH the same data\n",
        "# better use cross-validation (using decision tree model still)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz2m5CO6RKgF"
      },
      "outputs": [],
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJfCGlpcRKgF"
      },
      "outputs": [],
      "source": [
        "# can try to get the cross validation score using linear regression model \n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9BHzayBRKgF"
      },
      "source": [
        "**Note**: we specify `n_estimators=100` is the default value is going to change to 100 in Scikit-Learn now (not shown in book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewPjo7s5RKgG"
      },
      "outputs": [],
      "source": [
        "# try one more model - random forest regressor, but no cross validation\n",
        "# remember to set 'random_state' to 42, to ensure reproducibiiity\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN3SRqSnRKgG"
      },
      "outputs": [],
      "source": [
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8do05KT2RKgG"
      },
      "outputs": [],
      "source": [
        "# try the same using cross_val_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "display_scores(forest_rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNeNfdOzRKgG"
      },
      "outputs": [],
      "source": [
        "# now we have a few rounds for, say, Linear Regression cross_val_scores, let us see their statistics\n",
        "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "pd.Series(np.sqrt(-scores)).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YSRECDHRKgG"
      },
      "outputs": [],
      "source": [
        "# let us try Support Vector Machine (SVM)\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_reg = SVR(kernel=\"linear\")\n",
        "svm_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = svm_reg.predict(housing_prepared)\n",
        "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "svm_rmse = np.sqrt(svm_mse)\n",
        "svm_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0tVG6ARKgG"
      },
      "source": [
        "# Fine-Tune Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrVQcRDRKgG"
      },
      "source": [
        "## Grid Search\n",
        "When the search space is large (with various models and their respective ***hyperparameters*** (make sure you understand what are hyperparamters), we need to automatic the entire search process. The solution is **grid search**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLCiYDxmRKgH"
      },
      "outputs": [],
      "source": [
        "# Example of Grid Search using RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "    ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwg0v2SbRKgH"
      },
      "source": [
        "The best hyperparameter combination found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYvWD1jnRKgH"
      },
      "outputs": [],
      "source": [
        "# .best_params_ variable stores the best choise of hyperparameters\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldzhy8iXRKgH"
      },
      "outputs": [],
      "source": [
        "# .best_estimator_ reports also the model used (with the respective hyperparameters)\n",
        "grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCmNXYhERKgH"
      },
      "source": [
        "Let's look at the score of each hyperparameter combination tested during the grid search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVqNeGASRKgH"
      },
      "outputs": [],
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsB4GuWyRKgH"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(grid_search.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlgfDzsyRKgI"
      },
      "source": [
        "## Randomized Search\n",
        "Instead of the data scientist set the hyperparameters in grid search, how about randomly try various values for hyperparameters.  This is called ***Randomized Search***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjqgaEAURKgI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example of Randomized Search with Cross-Validation\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=1, high=200),\n",
        "        'max_features': randint(low=1, high=8),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdRtzD3lRKgI"
      },
      "outputs": [],
      "source": [
        "# Print the cross-validation results for various hyperparameters in each iteration\n",
        "# random search is controlled by the number of iteration 'n_iter', which was set to 10\n",
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Y8fU5iRKgI"
      },
      "source": [
        "## Analyze the Best Models and Their Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUJ0ub27RKgI"
      },
      "outputs": [],
      "source": [
        "# you can also analyse the feature importance of the best model\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IoJ6EWTRKgJ"
      },
      "outputs": [],
      "source": [
        "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
        "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOnKkpADRKgJ"
      },
      "source": [
        "## Evaluate Your System on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gKw4kAGRKgJ"
      },
      "outputs": [],
      "source": [
        "# we now have the best model and the best hyperparameters according to cross-validation\n",
        "# the remaining job is to try with the test data (that the model has never seen)\n",
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "# use stratified test data, remove the target column and copy this in a label array\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "# use the tranform pipeline to transform the test dataset\n",
        "# the model is already trained earlier, and all coeficients have been \"learned\"\n",
        "# now predict!!!\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL0LeVOqRKgJ"
      },
      "outputs": [],
      "source": [
        "# The final result after ALL THESE WORK\n",
        "final_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz70jTepRKgJ"
      },
      "source": [
        "We can compute a 95% confidence interval for the test RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTdxxINmRKgJ"
      },
      "outputs": [],
      "source": [
        "# Finally, computer the 95% condience level\n",
        "# with 95% chance that the RMSE of the predicted house value is withih [45893, 49774]\n",
        "# this can be compared with past experience from experts (without ML), and inform investment by the Company\n",
        "# job well done!!!!\n",
        "from scipy import stats\n",
        "\n",
        "confidence = 0.95\n",
        "squared_errors = (final_predictions - y_test) ** 2\n",
        "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "                         loc=squared_errors.mean(),\n",
        "                         scale=stats.sem(squared_errors)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAhzOgdGRKgK"
      },
      "source": [
        "We could compute the interval manually like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDEXTfwXRKgK"
      },
      "outputs": [],
      "source": [
        "m = len(squared_errors)\n",
        "mean = squared_errors.mean()\n",
        "tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\n",
        "tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
        "np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobeMxYMRKgK"
      },
      "source": [
        "Alternatively, we could use a z-scores rather than t-scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex2PJtmJRKgK"
      },
      "outputs": [],
      "source": [
        "zscore = stats.norm.ppf((1 + confidence) / 2)\n",
        "zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\n",
        "np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aniEvaBmRKgK"
      },
      "source": [
        "# Extra material\n",
        "The following are extra material not coverd in lecture or the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQVo0cMLRKgK"
      },
      "source": [
        "## A full pipeline with both preparation and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d1jWqdpRKgK"
      },
      "outputs": [],
      "source": [
        "full_pipeline_with_predictor = Pipeline([\n",
        "        (\"preparation\", full_pipeline),\n",
        "        (\"linear\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
        "full_pipeline_with_predictor.predict(some_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgIjGRKcRKgL"
      },
      "source": [
        "## Model persistence using joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsNy8yHFRKgL"
      },
      "outputs": [],
      "source": [
        "my_model = full_pipeline_with_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXPCVyb2RKgL"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(my_model, \"my_model.pkl\") # DIFF\n",
        "#...\n",
        "my_model_loaded = joblib.load(\"my_model.pkl\") # DIFF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ldWoG9RKgL"
      },
      "source": [
        "## Example SciPy distributions for `RandomizedSearchCV`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTWzL2X8RKgL"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import geom, expon\n",
        "geom_distrib=geom(0.5).rvs(10000, random_state=42)\n",
        "expon_distrib=expon(scale=1).rvs(10000, random_state=42)\n",
        "plt.hist(geom_distrib, bins=50)\n",
        "plt.show()\n",
        "plt.hist(expon_distrib, bins=50)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "02_end_to_end_machine_learning_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}