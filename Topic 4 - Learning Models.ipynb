{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN7pPuAu7FgN"
      },
      "source": [
        "**Chapter 4 â€“ Training Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CSeog4f7FgR"
      },
      "source": [
        "*This is the Python code notebook for Topic 4 - Learning Models by the textbook author with additional comments for ECIC,*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUFVdMB_7FgS"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rEbEjck7FgS"
      },
      "source": [
        "Standard setup for all our Colab Notebook which appears in each and every notebook for this course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ885X9V7FgT"
      },
      "outputs": [],
      "source": [
        "# Import the commonly used libraries and modules\n",
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures (i.e. /content/images/Topic_4)\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"Topic_4\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wCU5z-N7FgU"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNsCSxtv7FgV"
      },
      "source": [
        "## The Normal Equation\n",
        "This section will solve the Linear Regression in close-form using matrix inversion.  This is possible and sensible only if you have a small training data size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJfQqary7FgV"
      },
      "outputs": [],
      "source": [
        "# Note that .rand() generates rectangular distributed random numbers with range (0 to 1.0)\n",
        "# Similarly .randn() generates normally distributed (Gaussian) random numbers of same range\n",
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v97LiEES7FgW"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "save_fig(\"generated_data_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PN1Buva7FgX"
      },
      "outputs": [],
      "source": [
        "# This sets up the vector X for matrix inversion\n",
        "# Need to add x0=1 (see Eq 4.2 in textbook)\n",
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnKnNJMX7FgX"
      },
      "outputs": [],
      "source": [
        "theta_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvw7pyhr7FgX"
      },
      "outputs": [],
      "source": [
        "# Make a prediction for x=0 and x=2\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "y_predict  # this is the last datapoint on the red line, for reference only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLwImDuh7FgY"
      },
      "outputs": [],
      "source": [
        "plt.plot(X_new, y_predict, \"r-\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBT9OXlp7FgY"
      },
      "source": [
        "The figure in the book actually corresponds to the following code, with a legend and axis labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sDGX7VX7FgZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "save_fig(\"linear_model_predictions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbDDzpBs7FgZ"
      },
      "outputs": [],
      "source": [
        "# In fact, using Scikit-Learn is much easier - just .fit()!\n",
        "# \"lin_reg.intercept_\" and \"lin_reg.coef_\" are called instance variables\n",
        "# holding the internal states of the model (note the '_' at the end of name)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pC9I75f7FgZ"
      },
      "outputs": [],
      "source": [
        "# Use .predict() to make prediction ()\n",
        "lin_reg.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfB-Sl7y7Fga"
      },
      "source": [
        "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you could call directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "598M5k6Z7Fga"
      },
      "outputs": [],
      "source": [
        "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
        "theta_best_svd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeeKZktH7Fga"
      },
      "source": [
        "This function computes $\\mathbf{X}^+\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ-a7OPi7Fga"
      },
      "outputs": [],
      "source": [
        "# Material not covered in class, another option to compute the inverse of matrix\n",
        "np.linalg.pinv(X_b).dot(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFX13IiT7Fgb"
      },
      "source": [
        "# Gradient Descent\n",
        "## Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAzUncxs7Fgb"
      },
      "outputs": [],
      "source": [
        "# This code segment demonstrate Batch Gradient Descent, m = # of samples\n",
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization of theta 0 and 1\n",
        " \n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iy0ETQo7Fgb"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksYQ1Gec7Fgb"
      },
      "outputs": [],
      "source": [
        "X_new_b.dot(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mhNlfDT7Fgc"
      },
      "outputs": [],
      "source": [
        "# This subsegment of code simply plot out the path taken for Gradient Descent\n",
        "# for different values of training rate (eta = 0.02, 0.1 and 0.5 )\n",
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "        theta = theta - eta * gradients\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, 0, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXFxhliA7Fgc"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "save_fig(\"gradient_descent_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avfABBMs7Fgc"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "This code segment demonstrates Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcN8xHxi7Fgc"
      },
      "outputs": [],
      "source": [
        "theta_path_sgd = []\n",
        "m = len(X_b)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVNMfJON7Fgc"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        if epoch == 0 and i < 20:                    # not shown in the book\n",
        "            y_predict = X_new_b.dot(theta)           # not shown\n",
        "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
        "            plt.plot(X_new, y_predict, style)        # not shown\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_sgd.append(theta)                 # not shown\n",
        "\n",
        "plt.plot(X, y, \"b.\")                                 # not shown\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
        "plt.axis([0, 2, 0, 15])                              # not shown\n",
        "save_fig(\"sgd_plot\")                                 # not shown\n",
        "plt.show()                                           # not shown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e1flttVg7Fgf"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIMMFvhO7Fgg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJhb1yez7Fgg"
      },
      "outputs": [],
      "source": [
        "sgd_reg.intercept_, sgd_reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Vky0gj7Fgg"
      },
      "source": [
        "## Mini-batch gradient descent\n",
        "Demonstration of Mini-batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P21ZZRbU7Fgg"
      },
      "outputs": [],
      "source": [
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size]\n",
        "        yi = y_shuffled[i:i+minibatch_size]\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nv3yxG57Fgg"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XePbrmDS7Fgg"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "theta_path_mgd = np.array(theta_path_mgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2DZAvJh7Fgh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
        "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "save_fig(\"gradient_descent_paths_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83exW3Dk7Fgh"
      },
      "source": [
        "# Polynomial Regression\n",
        "Demonstration of Polynomial Regression - pay attention to the use of transformer to add a polynomial term as additional feature, then use Linear Regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWADvXRW7Fgh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmbMS0lL7Fgh"
      },
      "outputs": [],
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3E-1AWz7Fgh"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"quadratic_data_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1fv6uOH7Fgh"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jwxcv_O7Fgi"
      },
      "outputs": [],
      "source": [
        "X_poly[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTtizqpD7Fgi"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtnP8G7H7Fgi"
      },
      "outputs": [],
      "source": [
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"quadratic_predictions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nROc3UlQ7Fgi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"high_degree_polynomials_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w65iDvdQ7Fgi"
      },
      "source": [
        "# Learning Curves\n",
        "Demonstration the concept of Learning Curves to see if the model is underfitting or overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9NBuvcF7Fgi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train) + 1):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
        "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSaWzxeF7Fgj"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
        "save_fig(\"underfitting_learning_curves_plot\")   # not shown\n",
        "plt.show()                                      # not shown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7rSl7Ou7Fgj"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])           # not shown\n",
        "save_fig(\"learning_curves_plot\")  # not shown\n",
        "plt.show()                        # not shown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILAD8y4Z7Fgj"
      },
      "source": [
        "# Regularized Linear Models\n",
        "This code segment demonstrate the use of various regularization methods to ensure we do not overfit or underfit, as we do not know what order of polynomial should be used given the data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LqROhcW7Fgj"
      },
      "source": [
        "## Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufXFtvNY7Fgj"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY9O5UOy7Fgj"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n93lfHXq7Fgj"
      },
      "outputs": [],
      "source": [
        "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc48Bndr7Fgk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 4])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "save_fig(\"ridge_regression_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Wk3aut7Fgk"
      },
      "source": [
        "**Note**: to be future-proof, we set `max_iter=1000` and `tol=1e-3` because these will be the default values in Scikit-Learn 0.21."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLIjRUf-7Fgk"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ac3bKR7Fgk"
      },
      "source": [
        "## Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEq4YFEQ7Fgk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
        "\n",
        "save_fig(\"lasso_regression_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MnV3es07Fgk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0hOK3Tx7Fgl"
      },
      "source": [
        "## Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHN7QEtz7Fgl"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y__Fxtug7Fgl"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ab6XvJY7Fgl"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tA4cqCon7Fgl"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = deepcopy(sgd_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nza680A07Fgl"
      },
      "source": [
        "Create the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUdTh2V-7Fgm"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "save_fig(\"early_stopping_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFLN-ifn7Fgm"
      },
      "outputs": [],
      "source": [
        "best_epoch, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVdr6LOf7Fgm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E4Iql0Y7Fgm"
      },
      "outputs": [],
      "source": [
        "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
        "\n",
        "t1s = np.linspace(t1a, t1b, 500)\n",
        "t2s = np.linspace(t2a, t2b, 500)\n",
        "t1, t2 = np.meshgrid(t1s, t2s)\n",
        "T = np.c_[t1.ravel(), t2.ravel()]\n",
        "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
        "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
        "\n",
        "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
        "\n",
        "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
        "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
        "\n",
        "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
        "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
        "\n",
        "t_init = np.array([[0.25], [-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb5I3bt67Fgm"
      },
      "outputs": [],
      "source": [
        "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200):\n",
        "    path = [theta]\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta\n",
        "        theta = theta - eta * gradients\n",
        "        path.append(theta)\n",
        "    return np.array(path)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
        "for i, N, l1, l2, title in ((0, N1, 2., 0, \"Lasso\"), (1, N2, 0,  2., \"Ridge\")):\n",
        "    JR = J + l1 * N1 + l2 * 0.5 * N2**2\n",
        "    \n",
        "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
        "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
        "\n",
        "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
        "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
        "    levelsN=np.linspace(0, np.max(N), 10)\n",
        "    \n",
        "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
        "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
        "    path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
        "\n",
        "    ax = axes[i, 0]\n",
        "    ax.grid(True)\n",
        "    ax.axhline(y=0, color='k')\n",
        "    ax.axvline(x=0, color='k')\n",
        "    ax.contourf(t1, t2, N / 2., levels=levelsN)\n",
        "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
        "    ax.plot(0, 0, \"ys\")\n",
        "    ax.plot(t1_min, t2_min, \"ys\")\n",
        "    ax.set_title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
        "    ax.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
        "    ax.set_ylabel(r\"$\\theta_2$\", fontsize=16, rotation=0)\n",
        "\n",
        "    ax = axes[i, 1]\n",
        "    ax.grid(True)\n",
        "    ax.axhline(y=0, color='k')\n",
        "    ax.axvline(x=0, color='k')\n",
        "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
        "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
        "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
        "    ax.plot(0, 0, \"ys\")\n",
        "    ax.plot(t1_min, t2_min, \"ys\")\n",
        "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.axis([t1a, t1b, t2a, t2b])\n",
        "    if i == 1:\n",
        "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
        "\n",
        "save_fig(\"lasso_vs_ridge_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGOVgHGr7Fgn"
      },
      "source": [
        "# Logistic Regression\n",
        "This code segment demonstrates the use of Regression for classification, by turning probability estimate into class decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k4FHEVM7Fgn"
      },
      "source": [
        "## Decision Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6NnCpZN7Fgn"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "save_fig(\"logistic_function_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MWj75-97Fgn"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBV3Kj1-7Fgn"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7tmRuDb7Fgn"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y331UKqK7Fgn"
      },
      "source": [
        "**Note**: To be future-proof we set `solver=\"lbfgs\"` since this will be the default value in Scikit-Learn 0.22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlXB3Vg97Fgn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-LLF-0_7Fgo"
      },
      "outputs": [],
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtQKEXU7Fgo"
      },
      "source": [
        "The figure in the book actually is actually a bit fancier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7xGZREC7Fgo"
      },
      "outputs": [],
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "save_fig(\"logistic_regression_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMHvk0tV7Fgo"
      },
      "outputs": [],
      "source": [
        "decision_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWdkSKHq7Fgo"
      },
      "outputs": [],
      "source": [
        "log_reg.predict([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW7Dc_4_7Fgo"
      },
      "source": [
        "## Softmax Regression\n",
        "This code segment demonstrate how to do multiclass classification by regression.  We use the Iris flower dataset as example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc5zqMJx7Fgo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)\n",
        "\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "save_fig(\"logistic_regression_contour_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hmq8WE7Fgp"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxcW2DLC7Fgp"
      },
      "outputs": [],
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "save_fig(\"softmax_regression_contour_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecgyK69x7Fgp"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict([[5, 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHTvCxpE7Fgp"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict_proba([[5, 2]])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "04_training_linear_models(ECIC).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "1LqROhcW7Fgj",
        "Y3ac3bKR7Fgk",
        "p0hOK3Tx7Fgl",
        "Y__Fxtug7Fgl",
        "9k4FHEVM7Fgn"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}